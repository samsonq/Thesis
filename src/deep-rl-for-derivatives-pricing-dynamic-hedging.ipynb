{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport tensorflow as tf\nimport keras\nimport gym\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-02-13T06:35:50.131143Z","iopub.execute_input":"2022-02-13T06:35:50.131483Z","iopub.status.idle":"2022-02-13T06:35:57.345263Z","shell.execute_reply.started":"2022-02-13T06:35:50.131392Z","shell.execute_reply":"2022-02-13T06:35:57.344322Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class OptionEnv(gym.Env):\n    def __init__(self):\n        self.S0 = 100.0\n        self.K = 100.0\n        self.r = 0.02\n        self.sigma = 0.20\n        self.T = 1.0\n        self.N = 365    # 365 days\n        self.S1 = 0\n        self.reward = 0\n        self.day_step = 0    # from day 0 taking N steps to day N\n\n        self.action_space = gym.spaces.Discrete(2)         # 0: hold, 1:exercise\n        self.observation_space = gym.spaces.Box(low=np.array([0, 0]), high=np.array([np.inf, 1.0]), dtype=np.float32)      # S in [0, inf], tao in [0, 1]\n\n    def step(self, action):\n        if action == 1:        # exercise\n            reward = max(K-self.S1, 0.0) * np.exp(-self.r * self.T * (self.day_step/self.N))\n            done = True\n        else:       # hold\n            if self.day_step == self.N:    # at maturity\n                reward = max(self.K-self.S1, 0.0) * np.exp(-self.r * self.T)\n                done = True\n            else: # move to tomorrow\n                reward = 0\n                # lnS1 - lnS0 = (r - 0.5*sigma^2)*t + sigma * Wt\n                self.S1 = self.S1 * np.exp((self.r - 0.5 * self.sigma**2) * (self.T/self.N) + self.sigma * np.sqrt(self.T/self.N) * np.random.normal())\n                self.day_step += 1\n                done = False\n\n        tao = 1.0-self.day_step/self.N        # time to maturity, in unit of years\n        return np.array([self.S1, tao]), reward, done, {}\n\n    def reset(self):\n        self.day_step = 0\n        self.S1 = self.S0\n        tao = 1.0-self.day_step/self.N        # time to maturity, in unit of years\n        return [self.S1, tao]\n    \n    def render(self):\n        \"\"\"\n        make video\n        \"\"\"\n        pass\n\n    def close(self):\n        pass","metadata":{"execution":{"iopub.status.busy":"2022-02-13T06:35:57.346928Z","iopub.execute_input":"2022-02-13T06:35:57.347201Z","iopub.status.idle":"2022-02-13T06:35:57.363212Z","shell.execute_reply.started":"2022-02-13T06:35:57.347166Z","shell.execute_reply":"2022-02-13T06:35:57.362574Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"env = OptionEnv()\ns = env.reset()\n\nsim_prices = []\nsim_prices.append(s[0])\nfor i in range(365):\n    action = 0\n    s_next, reward, done, info = env.step(action)\n    sim_prices.append(s_next[0])\n\nplt.xlabel('Date')\nplt.ylabel('Stock Price')\nplt.plot(sim_prices)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T06:35:57.364256Z","iopub.execute_input":"2022-02-13T06:35:57.364578Z","iopub.status.idle":"2022-02-13T06:35:57.618790Z","shell.execute_reply.started":"2022-02-13T06:35:57.364550Z","shell.execute_reply":"2022-02-13T06:35:57.617937Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"!pip install tf_agents","metadata":{"execution":{"iopub.status.busy":"2022-02-13T06:35:57.620487Z","iopub.execute_input":"2022-02-13T06:35:57.620700Z","iopub.status.idle":"2022-02-13T06:36:10.570768Z","shell.execute_reply.started":"2022-02-13T06:35:57.620672Z","shell.execute_reply":"2022-02-13T06:36:10.569907Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from tf_agents.environments import  gym_wrapper           # wrap OpenAI gym\nfrom tf_agents.environments import tf_py_environment      # gym to tf gym\nfrom tf_agents.networks import q_network                  # Q net\nfrom tf_agents.agents.dqn import dqn_agent                # DQN Agent\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer      # replay buffer\nfrom tf_agents.trajectories import trajectory              # s->s' trajectory\nfrom tf_agents.utils import common                       # loss function","metadata":{"execution":{"iopub.status.busy":"2022-02-13T06:36:10.572528Z","iopub.execute_input":"2022-02-13T06:36:10.573025Z","iopub.status.idle":"2022-02-13T06:36:12.942746Z","shell.execute_reply.started":"2022-02-13T06:36:10.572910Z","shell.execute_reply":"2022-02-13T06:36:12.942013Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class RLHyperparameters:\n    # Hyper-parameters\n    num_iterations = 20000 # @param {type:\"integer\"}\n\n    collect_steps_per_iteration = 10  # @param {type:\"integer\"}\n    replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n    batch_size = 256  # @param {type:\"integer\"}\n\n    learning_rate = 1e-3  # @param {type:\"number\"}\n    num_eval_episodes = 10  # @param {type:\"integer\"}\n\n    eval_interval = 1000  # @param {type:\"integer\"}\n    log_interval = 200  # @param {type:\"integer\"}\n    \n    \nhyperparameters = RLHyperparameters","metadata":{"execution":{"iopub.status.busy":"2022-02-13T06:36:12.944337Z","iopub.execute_input":"2022-02-13T06:36:12.944902Z","iopub.status.idle":"2022-02-13T06:36:12.951742Z","shell.execute_reply.started":"2022-02-13T06:36:12.944855Z","shell.execute_reply":"2022-02-13T06:36:12.950984Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_env_gym = OptionEnv()\neval_env_gym = OptionEnv()\n\ntrain_env_wrap = gym_wrapper.GymWrapper(train_env_gym)\neval_env_wrap = gym_wrapper.GymWrapper(eval_env_gym)\n\ntrain_env  = tf_py_environment.TFPyEnvironment(train_env_wrap)\neval_env = tf_py_environment.TFPyEnvironment(eval_env_wrap)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T06:36:12.952839Z","iopub.execute_input":"2022-02-13T06:36:12.953313Z","iopub.status.idle":"2022-02-13T06:36:12.987941Z","shell.execute_reply.started":"2022-02-13T06:36:12.953263Z","shell.execute_reply":"2022-02-13T06:36:12.986783Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Define Q Net\nfc_layer_params = (100,)\n\nq_net = q_network.QNetwork(\n    train_env.observation_spec(),\n    train_env.action_spec(),\n    fc_layer_params=fc_layer_params)\n\n\n# Define DQN Agent\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=hyperparameters.learning_rate)\n\ntrain_step_counter = tf.Variable(0)\n\nagent = dqn_agent.DqnAgent(\n    train_env.time_step_spec(),\n    train_env.action_spec(),\n    q_network=q_net,\n    optimizer=optimizer,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=train_step_counter)\n\nagent.initialize()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T06:36:12.989417Z","iopub.execute_input":"2022-02-13T06:36:12.989756Z","iopub.status.idle":"2022-02-13T06:36:13.206751Z","shell.execute_reply.started":"2022-02-13T06:36:12.989711Z","shell.execute_reply":"2022-02-13T06:36:13.205747Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}